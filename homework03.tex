\documentclass{article}
\usepackage{graphicx} 
\usepackage{theJackPack}

\title{Homework 3 - Jack Brolin, Abhiram Nallamalli}
\author{Jack Brolin}
\date{July 2023}

\begin{document}

\maketitle

\begin{jacklist}
    \begin{framed} 
    \item [\textbf{P. 2}] Let $P=\left\{x \in \mathbb{R}^{3} \mid x_{1}+x_{2}+x_{3}=1, x \geq 0\right\}$ and consider the vector 
        $x=(0,0,1)$. Find the set of feasible directions at $x$.
    \end{framed}
    \begin{proof}
        Let $d = (a,b,c); a,b,c \in \mathbb{R}$. Then 
        \begin{align*}
            &(0 + \theta a) + (0 + \theta b ) + (1 + \theta c) = 1 \\
            \Rightarrow \quad &(a + b + c)\theta  = 0 \\
            \Rightarrow \quad &a + b + c = 0 
        \end{align*}
        and thus $c = -a - b$. Since $x = (0,0,1)$, we must require $a,b > 0$ so the shift meets the condition at $x \geq 0$. 
        Furthermore, we must also require $1 + \theta c \leq 0 \Rightarrow 1 \geq \theta(a+b) \Rightarrow \frac{1}{a+b} \geq \theta$.
        This is always satisfied by $a > 0$ or $b > 0$. We get our final set of feasible directions: 
        \[ 
            d = \{(a,b,-a-b) : a,b \geq 0 \}
        \] 
    \end{proof}

%P 3: Done
\newpage
    \begin{framed} 
    \item [\textbf{P. 3}] Consider the problem of minimizing $c^{\prime} x$ over a polyhedron $P$. Prove the following:
        \begin{itemize}
            \item [a.] A feasible solution $x$ is optimal if and only if $c^{\prime} d \geq \textbf{0}$ for every feasible direction 
                $d$ at $x$. 
            \item [b.] A feasible solution $x$ is the unique optimal solution if and only if $c^{\prime} d> \textbf{0}$ 
                for every nonzero feasible direction $d$ at $x$. 
        \end{itemize}
    \end{framed}
    \begin{itemize}
        \item [a.] 
            \begin{proof}
                Assume $x^\star$ is an optimal solution and let $d$ be any arbitrary feasible direction vector at $x^\star$.
                Then $x^\star + \theta d \in P$ for some $\theta > 0$. For this new vector, the optimality of $x^\star$ implies 
                \begin{align*}
                    &c^\prime x^\star \leq c^\prime(x^\star + \theta d) \\
                    \Rightarrow \quad &c^\prime x^\star \leq c^\prime x^\star + \theta c^\prime d \\
                    \Rightarrow \quad &\textbf{0} \leq \theta c^\prime d \\
                    \Rightarrow \quad &\textbf{0} \leq c^\prime d
                \end{align*}
                Now assume $c^\prime d \geq \textbf{0}$ for all feasible directions $d$ at $x$. Let $y := x + \theta d$ for any $d$.
                Then $\theta d = y - x$ and we see 
                \[ 
                    \textbf{0} \leq \theta c^\prime d = \theta c^\prime(y-x) = \theta c^\prime y - \theta c^\prime x \Rightarrow 
                    \theta c^\prime y \geq \theta c^\prime x \Rightarrow c^\prime y \geq c^\prime x
                \] 
                Showing that $x$ is indeed optimal.
            \end{proof}
        \item [b.] 
            \begin{proof}
                Assume $x^\star$ is the unique optimal solution and let $d$ be any arbitrary feasible direction vector at $x^\star$.
                Then $x^\star + \theta d \in P$ for some $\theta > 0$. For this new vector, the optimality of $x^\star$ implies 
                \begin{align*}
                    &c^\prime x^\star < c^\prime(x^\star + \theta d) \\
                    \Rightarrow \quad &c^\prime x^\star < c^\prime x^\star + \theta c^\prime d \\
                    \Rightarrow \quad &\textbf{0} < \theta c^\prime d \\
                    \Rightarrow \quad &\textbf{0} < c^\prime d
                \end{align*}
                Now assume $c^\prime d > \textbf{0}$ for all nonzero feasible directions $d$ at $x$. Let $y := x + \theta d$ for any $d$.
                Then $\theta d = y - x$ and we see 
                \[ 
                    \textbf{0} < \theta c^\prime d = \theta c^\prime(y-x) = \theta c^\prime y - \theta c^\prime x \Rightarrow 
                    \theta c^\prime y > \theta c^\prime x \Rightarrow c^\prime y > c^\prime x
                \] 
                Showing that $x$ is indeed the unique optimal solution.
            \end{proof}
    \end{itemize}

\newpage
    \begin{framed} 
    \item [\textbf{P. 4}] Let $x$ be an element of the standard form polyhedron 
        $P=\left\{x \in \mathbb{R}^{n} \mid A x=b, x \geq 0\right\}$. Prove that a vector $d \in \mathbb{R}^{n}$ is a feasible 
        direction at $x$ if and only if $A d=0$ and $d_{i} \geq 0$ for every $i$ such that $x_{i}=0$.
    \end{framed}
    \begin{proof}
        ($\Rightarrow$) Assume $d$ is a feasible direction at $x$. We are only concerned with the edges of the $P$ so it must hold that if 
        $x + \theta d \in P; A(x+\theta d) = Ax + \theta Ad = b \Rightarrow Ad = 0$ as $Ax = b, \theta > 0$. Furthermore, 
        from feasibilty of $d$ we get $x + \theta d \geq 0$ and we simply look component wise to get $x_i + \theta d_i \geq 0$ but where
        $x_i = 0$, it must hold that $d_i \geq 0$ as $\theta > 0$. \\
        ($\Leftarrow$) Assume $Ad = \textbf{0}$ and $d_i \geq 0$ when $x_i = 0$. Note that $Ad = \textbf{0}$ 
        ensures $A(x + \theta d) = b$ never gets violated so we construct $\theta$ such that 
        $x + \theta d \geq \textbf{0}$ i.e. $x_i + \theta d_i \geq 0$. We have three cases:
        \begin{itemize}
            \item If $x_i = 0$ then $d_i \geq 0$ by assumption 
            \item If $x_i > 0$ and $d_i > 0$, there is no restriction on $\theta$ 
            \item If $x_i > 0$ and $d_i < 0$, define $ \displaystyle \theta := \min_{i:d_i<0}\left\{ \frac{-x_i}{d_i}\right\}$ so
                $ \displaystyle \theta \leq \frac{-x_i}{d_i} \Rightarrow \theta d_i \geq -x_i \Rightarrow x_i + \theta d_i \geq 0$ 
                for all relevant $i$
        \end{itemize}
        In each case, $x + \theta d \geq \textbf{0}$ and thus $d$ is a feasible direction at $x$. 
    \end{proof}

\newpage
    \begin{framed} 
    \item [\textbf{P. 7}] Consider a feasible solution $x$ to the standard form problem 
        \begin{align*}
            \text{minimize } & c^{\prime} x \\
            \text { subject to } & A x=b \\
            & x \geq 0,
        \end{align*}
        and let $Z=\left\{i: x_{i}=0\right\}$. Show that $x$ is an optimal solution if and only if the linear programming problem
        \begin{align*}
            \text{minimize } & c^{\prime} d \\
            \text { subject to } & A d=0 \quad \\
            & d_{i} \geq 0, \quad i \in Z,
        \end{align*}
        has an optimal cost of zero.
    \end{framed}
    \begin{proof}
        Suppose $x$ is an optimal solution. \textbf{P. 3} tells us that $c^\prime \geq 0$ for all feasible direction vectors $d$ at $x$.
        Clearly, the min of this is 0. For the sufficient condition, suppose $c^\prime d$ has  an optimal cost of zero. From 
        \textbf{P. 4} we know $d$ is a feasible direction at $x$. Furthermore, $c^\prime d \leq 0$ as the optimal cost is 0. Again,
        \textbf{P. 3} comes to the rescue with the result that this condition is sufficent for $x$ to be optimal. 
    \end{proof}

\newpage
    \begin{framed} 
    \item [\textbf{P. 9}] Consider the problem 
        \[\begin{array}{rc}
            \operatorname{minimize} & -2 x_{1}-x_{2} \\
            \text { subject to } & x_{1}-x_{2} \leq 2 \\
            & x_{1}+x_{2} \leq 6 \\
            & x_{1}, x_{2} \geq 0
        \end{array}\]
    \end{framed}
    We first find the equivalent standard form representation: 
    \begin{align*}
        \text{min } -&2x_1 - x_2 \\
        \text{s.t. } &x_1 - x_2 + x_3 = 2 \\
                     &x_1 + x_2 + x_4 = 6 \\
                     & x \geq 0
    \end{align*}
    and we can immediately see that $x = (0,0,2,6)$ is a basic feasible solution and so we let $B(1) = 3$ and $B(2) = 4$ giving a 
    basis matrix of $I$. Filling in the first tableau is trivial, as the necessary conditions arise very naturally: 
    \begin{center}
        \begin{tabular}{|c|cccc|}
           \hline 
           0 & -2 & -1 & 0 & 0 \\
           \hline 
           2 & 1 & -1 & 1 & 0 \\
           6 & 1 & 1 & 0 & 1 \\
           \hline
        \end{tabular}
    \end{center}
    Both relevant values in the $0^{\text{th}}$ are negative so we make an arbitrary choice of the second column $x_2$ and get
    $u = (-1,1)$. We only need to consider the ratio between $ \displaystyle \frac{x_{B(2)}}{u_2}$ as $u_1 < 0$ to get $\ell = 2$. 
    This gives our pivot element and choice of vector to enter the basis. Computing our version of Echelon Reduction we get the next 
    tableau: 
    \begin{center}
        \begin{tabular}{|c|cccc|}
            \hline
            6&-1&0&0&1 \\
            \hline 
            8&2&0&1&1\\
            6&1&1&0&1\\
            \hline
        \end{tabular}
    \end{center} 
    We only have one choice for our pivot column and we get $u = (2,1)$ with ratios 
    \begin{align*}
        &\frac{x_{B(1)}}{u_1} = \frac{8}{2} = 4 \\
        &\frac{x_{(2)}}{u_2} = \frac{6}{1} = 6 
    \end{align*}
    So we let $\ell = 1$ which corresponds to $x_3$. Now, with 2 as our pivot variable, we can run the next round of reductions and 
    get the last tableau: 
    \begin{center}
        \begin{tabular}{|c|cccc|}
            \hline
            10&0&0&$\rfrac{1}{2}$&$\rfrac{3}{2}$ \\
            \hline
            4&1&0&$\rfrac{1}{2}$&$\rfrac{1}{2}$ \\
            2&0&1&$\rfrac{1}{2}$&$\rfrac{3}{2}$ \\
            \hline
        \end{tabular}
    \end{center}
    With no more negative values in the reduced cost vector, we are sure that we are done and have achieved the optimal solution. 
    At every step we can do a confidence check and compute the values of the needed vector and make sure that each is a basic feasible 
    solution. We know at all times what row corresponds to what component by considering the components we choice to enter the basis,
    but in practice we just look at the unit vector in specific columns to get the corresponding components. This allows us to see our 
    final answer of $x = (4,2)$ which is chosen to minimize the above cost vector. 
\end{jacklist}

\end{document}
